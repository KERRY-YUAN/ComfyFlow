# File: app.py
# Version 4.0.0 (NodeBridge Integration, Workflow Listing, Enhanced ComfyUI Listener)

from flask import Flask, render_template, request, jsonify, send_from_directory
# Use gevent for async mode with SocketIO / 使用 gevent 作为 SocketIO 的异步模式
from gevent import monkey
monkey.patch_all()
from flask_socketio import SocketIO, emit, join_room, leave_room, Namespace
from flask_cors import CORS
import os
import json
import uuid
import websocket # For main ComfyUI connection AND bridge connection (managed by Node) / 用于主 ComfyUI 连接和桥接连接（由 Node 管理）
import threading
import time
import base64
from io import BytesIO
from PIL import Image
import sys
import logging # Import logging module
import numpy as np # Required for tensor_to_pil

# --- Logging Setup ---
# Setup basic logging / 设置基本日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log = logging.getLogger('ComfyFlowApp') # Create a logger instance / 创建记录器实例

# --- Configuration / 配置 ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BASE_DIR)
STATIC_FOLDER = os.path.join(BASE_DIR, 'static')
TEMPLATE_FOLDER = os.path.join(BASE_DIR, 'templates')

# ComfyUI Configuration (Ensure these are correct) / ComfyUI 配置（确保这些是正确的）
# IMPORTANT: Please double-check this path carefully! / 重要提示：请仔细复核此路径！
# Using raw string and normalizing the path / 使用原始字符串并规范化路径
COMFYUI_WORKFLOWS_PATH = os.path.normpath(r"D:\Program\ComfyUI_Program\ComfyUI\user\default\workflows")
# IMPORTANT: Ensure this path is correct and accessible / 重要提示：确保存储路径正确且可访问
COMFYUI_INPUT_PATH = os.path.normpath(r"D:\Program\ComfyUI_Program\ComfyUI\input")
# IMPORTANT: Ensure this path is correct and accessible / 重要提示：确保存储路径正确且可访问
COMFYUI_OUTPUT_PATH = os.path.normpath(r"D:\Program\ComfyUI_Program\ComfyUI\output")
# IMPORTANT: Ensure this matches your ComfyUI API address / 重要提示：确保这与您的 ComfyUI API 地址匹配
COMFYUI_API_ADDRESS = "127.0.0.1:8188" # Example: "127.0.0.1:8188" / 示例："127.0.0.1:8188"

# Bridge Namespace WebSocket URL (used by NodeBridge.py) / 桥接命名空间 WebSocket URL（由 NodeBridge.py 使用）
# This app hosts this namespace / 此应用程序托管此命名空间
BRIDGE_NAMESPACE = '/bridge'

# --- Flask App Setup ---
app = Flask(__name__, template_folder=TEMPLATE_FOLDER, static_folder=STATIC_FOLDER)
app.config['SECRET_KEY'] = 'your_very_secret_key_v4_bridge_change_me!' # Change this! / 更改这个！
CORS(app)
# Use gevent for robust SocketIO / 使用 gevent 以获得健壮的 SocketIO
socketio = SocketIO(app, async_mode='gevent', cors_allowed_origins="*", logger=True, engineio_logger=True) # Enable SocketIO logging / 启用 SocketIO 日志

# --- Data Structures ---
# Stores mapping from client's SID to their current prompt info / 存储从客户端 SID 到其当前提示信息的映射
client_prompt_map = {} # { client_id: {'prompt_id': prompt_id, 'workflow_data': workflow_data} }
# Stores mapping from prompt_id back to the client's SID / 存储从 prompt_id 回到客户端 SID 的映射
prompt_client_map = {} # { prompt_id: client_id }
# Stores pending data requests FROM NodeBridge TO Frontend, waiting for frontend response / 存储从 NodeBridge 到前端的待处理数据请求，等待前端响应
# Key: unique request_id generated by NodeBridge / 键：由 NodeBridge 生成的唯一 request_id
pending_node_requests = {}
# { request_id: {'prompt_id': ..., 'node_id':..., 'client_id':..., 'mode':..., 'node_sid':..., 'timestamp': ...} }

# --- Helper Functions ---
def tensor_to_pil(tensor):
    """Converts an image tensor (N, H, W, C) [0,1] float to a list of PIL Images."""
    """将图像张量 (N, H, W, C) [0,1] 浮点数转换为 PIL 图像列表。"""
    if tensor is None or not isinstance(tensor, torch.Tensor):
        log.warning("tensor_to_pil received None or non-Tensor input.")
        return []
    try:
        # Move tensor to CPU and convert to numpy array / 将张量移动到 CPU 并转换为 numpy 数组
        images = tensor.cpu().numpy()
        # Denormalize from [0, 1] to [0, 255] and convert to uint8 / 从 [0, 1] 反归一化到 [0, 255] 并转换为 uint8
        images = (images * 255).clip(0, 255).astype(np.uint8)

        # Handle different tensor dimensions / 处理不同的张量维度
        if images.ndim == 3: # Single image (H, W, C) / 单个图像 (H, W, C)
            return [Image.fromarray(images)]
        elif images.ndim == 4: # Batch of images (N, H, W, C) / 一批图像 (N, H, W, C)
            return [Image.fromarray(img) for img in images]
        else:
            log.warning(f"Unexpected tensor shape for PIL conversion: {images.shape}")
            return []
    except Exception as e:
        log.error(f"Error converting tensor to PIL: {e}", exc_info=True)
        return []

def pil_to_base64(pil_image, image_format='PNG'):
    """Converts a PIL image to a Base64 Data URL."""
    """将 PIL 图像转换为 Base64 数据 URL。"""
    if not isinstance(pil_image, Image.Image):
        log.error("pil_to_base64 received non-PIL image input.")
        return None
    try:
        buffered = BytesIO()
        # Handle transparency and format / 处理透明度和格式
        img_format_upper = image_format.upper()
        if img_format_upper not in ['PNG', 'JPEG', 'WEBP']:
            img_format_upper = 'PNG' # Default to PNG / 默认为 PNG

        save_format = img_format_upper
        mime_type = f'image/{save_format.lower()}'

        # Ensure correct mode for saving / 确保保存时模式正确
        if save_format == 'JPEG' and pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        elif save_format == 'PNG' and pil_image.mode not in ['RGB', 'RGBA', 'L']:
             # Try converting common modes to RGB for PNG compatibility / 尝试将常见模式转换为 RGB 以兼容 PNG
            try:
                 pil_image = pil_image.convert('RGB')
                 log.warning(f"Converted PIL image mode {pil_image.mode} to RGB for PNG saving.")
            except Exception as conv_err:
                 log.error(f"Could not convert PIL image mode {pil_image.mode} for PNG. Error: {conv_err}")
                 return None # Cannot save in this state / 无法在此状态下保存

        pil_image.save(buffered, format=save_format)
        img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        return f"data:{mime_type};base64,{img_base64}"
    except Exception as e:
        log.error(f"Error converting PIL image to base64: {e}", exc_info=True)
        return None

def load_workflow_safely(workflow_key):
    """Loads a workflow JSON file safely, preventing path traversal."""
    """安全地加载工作流 JSON 文件，防止路径遍历。"""
    if not workflow_key or '..' in workflow_key or workflow_key.startswith('/'):
        log.error(f"Invalid or potentially unsafe workflow key requested: {workflow_key}")
        return None, "Invalid workflow key."

    try:
        # Ensure the base path is absolute and normalized / 确保基本路径是绝对且规范化的
        workflows_base_path_abs = os.path.abspath(COMFYUI_WORKFLOWS_PATH)
        # Join and normalize the requested path / 连接并规范化请求的路径
        workflow_path_abs = os.path.normpath(os.path.join(workflows_base_path_abs, workflow_key))

        # Security Check: Ensure the final path is still within the base directory / 安全检查：确保最终路径仍在基本目录内
        if not workflow_path_abs.startswith(workflows_base_path_abs):
            log.error(f"Security Alert: Attempted path traversal for workflow key: {workflow_key}")
            return None, "Invalid workflow path (Path Traversal Attempt)."

        if not os.path.isfile(workflow_path_abs):
            log.error(f"Workflow file not found at resolved path: {workflow_path_abs}")
            return None, f"Workflow file not found: {workflow_key}"

        log.info(f"Loading workflow from: {workflow_path_abs}")
        with open(workflow_path_abs, 'r', encoding='utf-8') as f:
            workflow_data = json.load(f)
        return workflow_data, None # Return data and no error / 返回数据且无错误

    except json.JSONDecodeError as e:
        log.error(f"Invalid JSON in workflow file: {workflow_path_abs} - {e}")
        return None, f"Invalid JSON in workflow file: {workflow_key}"
    except Exception as e:
        log.error(f"Error reading workflow file {workflow_key}: {e}", exc_info=True)
        return None, "Server error reading workflow file."

# --- ComfyUI Main WebSocket Listener ---
def queue_comfyui_prompt(prompt_data, client_id, prompt_id):
    """Connects to ComfyUI main WS, sends the prompt, and listens for relevant events."""
    """连接到 ComfyUI 主 WebSocket，发送提示，并监听相关事件。"""
    ws = None
    comfyui_ws_url = f"ws://{COMFYUI_API_ADDRESS}/ws?clientId={client_id}"
    log.info(f"[{prompt_id}] Connecting to ComfyUI Main WS: {comfyui_ws_url}")

    try:
        # Use short timeout for connection, maybe retry needed / 对连接使用短超时，可能需要重试
        ws = websocket.create_connection(comfyui_ws_url, timeout=10)
        log.info(f"[{prompt_id}] ComfyUI Main WS connected successfully.")

        # Inject context (_prompt_id, _node_id) into NodeBridge nodes / 将上下文（_prompt_id, _node_id）注入 NodeBridge 节点
        modified_prompt = prompt_data.copy() # Use copy to avoid modifying original / 使用副本以避免修改原始数据
        nodes_to_inject = ["NodeBridge_Input", "NodeBridge_Output"] # Nodes needing context / 需要上下文的节点
        output_node_id_in_workflow = None # Track the output node ID / 跟踪输出节点 ID
        for node_id, node_info in modified_prompt.items():
            class_type = node_info.get("class_type")
            if class_type in nodes_to_inject:
                if "inputs" not in node_info: node_info["inputs"] = {}
                # Ensure values are strings / 确保值是字符串
                node_info["inputs"]["_prompt_id"] = str(prompt_id)
                node_info["inputs"]["_node_id"] = str(node_id)
                log.info(f"[{prompt_id}] Injected context into node {node_id} ({class_type})")
                if class_type == "NodeBridge_Output":
                    output_node_id_in_workflow = node_id

        log.info(f"[{prompt_id}] Queuing prompt for client {client_id}")
        ws.send(json.dumps({'prompt': modified_prompt, 'client_id': client_id}))

        # --- Listener Loop ---
        execution_completed = False
        while not execution_completed:
            message_str = None
            try:
                # Set a reasonable timeout for receiving messages / 为接收消息设置合理的超时
                message_str = ws.recv()
                if not message_str:
                    log.warning(f"[{prompt_id}] ComfyUI Main WS received empty message, breaking.")
                    break
            except websocket.WebSocketTimeoutException:
                 log.warning(f"[{prompt_id}] ComfyUI Main WS receive timeout, checking connection.")
                 try: # Send a ping to check if connection is still alive / 发送 ping 以检查连接是否仍然活动
                     ws.ping()
                     continue # Continue listening if ping succeeds / 如果 ping 成功，则继续监听
                 except Exception as ping_err:
                     log.error(f"[{prompt_id}] ComfyUI Main WS connection lost (ping failed: {ping_err}).")
                     break # Break loop if ping fails / 如果 ping 失败则中断循环
            except websocket.WebSocketConnectionClosedException:
                 log.error(f"[{prompt_id}] ComfyUI Main WS connection closed unexpectedly.")
                 break # Break loop if connection closed / 如果连接关闭则中断循环
            except Exception as recv_err:
                 log.error(f"[{prompt_id}] Error receiving from ComfyUI Main WS: {recv_err}", exc_info=True)
                 break # Break on other receive errors / 发生其他接收错误时中断

            # --- Process Received Message ---
            try:
                message = json.loads(message_str)
                msg_type = message.get('type')
                msg_data = message.get('data', {})
                exec_prompt_id = msg_data.get('prompt_id', None) # Get prompt_id where available / 在可用时获取 prompt_id

                # Ignore messages for other prompts / 忽略其他提示的消息
                if exec_prompt_id and exec_prompt_id != prompt_id:
                    # log.debug(f"Ignoring WS message for different prompt {exec_prompt_id}")
                    continue

                # --- Handle specific message types ---
                if msg_type == 'status':
                    status_info = msg_data.get('status', {})
                    queue_remaining = status_info.get('execinfo', {}).get('queue_remaining', 0)
                    log.info(f"[{prompt_id}] Status update: Queue remaining = {queue_remaining}")
                    socketio.emit('status_update', {'status': f"队列 Queue: {queue_remaining}"}, room=client_id)

                elif msg_type == 'execution_start':
                     if msg_data.get('prompt_id') == prompt_id:
                         log.info(f"[{prompt_id}] Execution started.")
                         socketio.emit('status_update', {'status': "执行开始 Execution Started..."}, room=client_id)

                elif msg_type == 'executing':
                    exec_node_id = msg_data.get('node')
                    if exec_node_id is not None: # Executing a specific node / 正在执行特定节点
                        node_title = modified_prompt.get(exec_node_id, {}).get('_meta', {}).get('title', f'Node {exec_node_id}')
                        log.info(f"[{prompt_id}] Executing node: {node_title} ({exec_node_id})")
                        socketio.emit('status_update', {'status': f"执行节点 Executing: {node_title}"}, room=client_id)
                    else: # Node is None, usually means the current prompt finished execution phase / Node 为 None，通常表示当前提示已完成执行阶段
                        log.info(f"[{prompt_id}] Execution phase finished signal.")
                        # This signal might indicate completion if no output node exists or was missed / 如果没有输出节点存在或被错过，此信号可能表示完成
                        # If we know the output node ID, we wait specifically for its 'executed' message / 如果我们知道输出节点 ID，我们将专门等待其“executed”消息
                        if output_node_id_in_workflow is None:
                             log.warning(f"[{prompt_id}] Execution phase finished, but no NodeBridge_Output found in workflow. Assuming completion.")
                             execution_completed = True # Mark as completed / 标记为已完成
                        else:
                             socketio.emit('status_update', {'status': "等待输出节点 Waiting for output node..."}, room=client_id)


                elif msg_type == 'executed':
                    executed_node_id = msg_data.get('node')
                    log.info(f"[{prompt_id}] Node {executed_node_id} executed.")

                    # Check if it's the tracked NodeBridge_Output node / 检查它是否是跟踪的 NodeBridge_Output 节点
                    if executed_node_id == output_node_id_in_workflow:
                        log.info(f"[{prompt_id}] Detected NodeBridge_Output execution ({executed_node_id}). Processing results.")
                        outputs = msg_data.get('outputs', {})
                        final_images_base64 = []

                        if 'images' in outputs:
                            log.info(f"[{prompt_id}] Output images found in node {executed_node_id}: {len(outputs['images'])}")
                            for img_info in outputs['images']:
                                filename = img_info.get('filename')
                                subfolder = img_info.get('subfolder', '')
                                img_type = img_info.get('type', 'output') # 'output' or 'temp' or 'input' / 'output' 或 'temp' 或 'input'
                                if not filename:
                                     log.warning(f"[{prompt_id}] Image info missing filename: {img_info}")
                                     continue

                                # Determine full path based on type / 根据类型确定完整路径
                                base_path = COMFYUI_OUTPUT_PATH
                                if img_type == 'input': base_path = COMFYUI_INPUT_PATH
                                # Assuming 'temp' files are also in OUTPUT for simplicity, adjust if needed / 为简单起见，假设“temp”文件也在 OUTPUT 中，如果需要请调整
                                elif img_type == 'temp': base_path = COMFYUI_OUTPUT_PATH

                                img_path = os.path.normpath(os.path.join(base_path, subfolder, filename))
                                log.info(f"[{prompt_id}] Attempting to process output image: {img_path}")

                                try:
                                    if os.path.exists(img_path) and os.path.isfile(img_path):
                                        with Image.open(img_path) as img:
                                             base64_data = pil_to_base64(img, image_format=img.format or 'PNG')
                                             if base64_data:
                                                 final_images_base64.append(base64_data)
                                                 log.info(f"[{prompt_id}] Successfully processed and encoded image: {filename}")
                                             else:
                                                 log.error(f"[{prompt_id}] Failed to encode image to base64: {filename}")
                                    else:
                                        log.error(f"[{prompt_id}] Output image file not found or is not a file: {img_path}")
                                except Exception as e:
                                    log.error(f"[{prompt_id}] Error processing output image file {filename}: {e}", exc_info=True)

                            if final_images_base64:
                                log.info(f"[{prompt_id}] Sending {len(final_images_base64)} images to client {client_id}.")
                                socketio.emit('render_result', {'images': final_images_base64}, room=client_id)
                            else:
                                log.warning(f"[{prompt_id}] NodeBridge_Output {executed_node_id} executed but no images were successfully processed.")
                                socketio.emit('render_error', {'message': 'Output node ran, but failed to process result images.'}, room=client_id)
                        else:
                            log.warning(f"[{prompt_id}] NodeBridge_Output {executed_node_id} executed but no 'images' key in output data.")
                            socketio.emit('render_error', {'message': 'Output node ran, but produced no image data.'}, room=client_id)

                        execution_completed = True # Mark as completed after processing output / 处理完输出后标记为已完成
                        log.info(f"[{prompt_id}] Task marked completed. Breaking listener loop.")
                        break # Task finished, break loop / 任务完成，中断循环

                elif msg_type == 'progress':
                    progress = msg_data.get('value', 0)
                    total = msg_data.get('max', 0)
                    percent = int((progress / total) * 100) if total > 0 else 0
                    # Optional: Add node title to progress if needed / 可选：如果需要，将节点标题添加到进度中
                    # log.debug(f"[{prompt_id}] Progress: {progress}/{total} ({percent}%)")
                    socketio.emit('progress_update', {'progress': progress, 'total': total, 'percent': percent}, room=client_id)

            except json.JSONDecodeError:
                 log.warning(f"[{prompt_id}] ComfyUI Main WS received non-JSON message: {message_str}")
            except Exception as e:
                 log.error(f"[{prompt_id}] Error processing ComfyUI Main WS message: {e}", exc_info=True)
                 # Consider notifying client of processing error / 考虑通知客户端处理错误
                 socketio.emit('render_error', {'message': f'Error processing ComfyUI message: {e}'}, room=client_id)

    except websocket.WebSocketException as e:
        log.error(f"[{prompt_id}] ComfyUI Main WS Error: {e}", exc_info=True)
        socketio.emit('render_error', {'message': f'ComfyUI Connection Error: {e}'}, room=client_id)
    except ConnectionRefusedError:
        log.error(f"[{prompt_id}] Connection to ComfyUI Main WS refused.")
        socketio.emit('render_error', {'message': 'Connection to ComfyUI refused. Is ComfyUI running?'}, room=client_id)
    except Exception as e:
        log.error(f"[{prompt_id}] Unexpected error in ComfyUI listener thread: {e}", exc_info=True)
        socketio.emit('render_error', {'message': f'An unexpected server error occurred: {e}'}, room=client_id)
    finally:
        if ws and ws.connected:
            try:
                ws.close()
                log.info(f"[{prompt_id}] ComfyUI Main WS connection closed.")
            except Exception as e:
                log.error(f"[{prompt_id}] Error closing ComfyUI Main WS: {e}")
        # Clean up prompt mapping after listener finishes / 监听器完成后清理提示映射
        owner_client = prompt_client_map.pop(prompt_id, None)
        if owner_client and owner_client in client_prompt_map:
            # Verify it's the correct prompt before deleting / 在删除前验证它是否是正确的提示
            if client_prompt_map[owner_client].get('prompt_id') == prompt_id:
                 del client_prompt_map[owner_client]
        log.info(f"[{prompt_id}] Cleaned up mappings.")
        # Send a final idle status / 发送最终空闲状态
        if owner_client:
             socketio.emit('status_update', {'status': "空闲 Idle"}, room=owner_client)


# --- Bridge Namespace for Node Communication ---
class BridgeNamespace(Namespace):
    """Handles WebSocket communication specifically for NodeBridge nodes."""
    """专门处理 NodeBridge 节点的 WebSocket 通信。"""
    def on_connect(self):
        # This SID belongs to the NodeBridge node's connection / 此 SID 属于 NodeBridge 节点的连接
        log.info(f"[Bridge] Node connected: {request.sid}")

    def on_disconnect(self):
        log.warning(f"[Bridge] Node disconnected: {request.sid}")
        # Clean up any pending requests associated with this node's sid / 清理与此节点 sid 关联的任何待处理请求
        requests_to_remove = []
        for req_id, req_info in list(pending_node_requests.items()): # Iterate over a copy / 迭代副本
            if req_info.get('node_sid') == request.sid:
                requests_to_remove.append(req_id)
        for req_id in requests_to_remove:
             if req_id in pending_node_requests:
                 log.warning(f"[Bridge] Cleaning up pending request {req_id} due to node disconnect.")
                 # Notify the frontend client that the node disconnected / 通知前端客户端节点已断开连接
                 client_id_to_notify = pending_node_requests[req_id].get('client_id')
                 if client_id_to_notify:
                     socketio.emit('render_error',
                                   {'message': '交互节点意外断开 (Bridge node disconnected unexpectedly)'},
                                   room=client_id_to_notify)
                 del pending_node_requests[req_id]

    def on_request_data_from_node(self, data):
        """Receives data request from NodeBridge_Input and relays it to the frontend."""
        """接收来自 NodeBridge_Input 的数据请求并将其转发给前端。"""
        node_sid = request.sid
        request_id = data.get('request_id')
        prompt_id = data.get('prompt_id')
        node_id = data.get('node_id')
        mode = data.get('mode')

        log.info(f"[Bridge] <= Received data request from node {node_sid}: req={request_id}, p={prompt_id}, n={node_id}, m={mode}")

        if not all([request_id, prompt_id, node_id, mode]):
            log.error(f"[Bridge] Incomplete data request from node {node_sid}: {data}")
            # Send error back to the node immediately / 立即将错误发送回节点
            emit('data_response_for_node', {'request_id': request_id, 'error': 'Incomplete request data received by backend.'}, room=node_sid)
            return

        # Find the corresponding frontend client using prompt_id / 使用 prompt_id 查找对应的前端客户端
        client_id = prompt_client_map.get(prompt_id)

        if not client_id:
            log.error(f"[Bridge] Could not find frontend client for prompt_id {prompt_id} (request {request_id} from node {node_sid}).")
            emit('data_response_for_node', {'request_id': request_id, 'error': 'Frontend client mapping not found by backend.'}, room=node_sid)
            return

        # Store the pending request, associating it with the node's SID / 存储待处理请求，并将其与节点的 SID 关联
        pending_node_requests[request_id] = {
            'request_id': request_id,
            'prompt_id': prompt_id,
            'node_id': node_id,
            'client_id': client_id, # Frontend client SID / 前端客户端 SID
            'mode': mode,
            'node_sid': node_sid, # Node connection SID / 节点连接 SID
            'timestamp': time.time()
        }
        log.info(f"[Bridge] Stored pending request {request_id} for node {node_sid}")

        # Relay the request to the specific frontend client via the main namespace / 通过主命名空间将请求转发给特定的前端客户端
        log.info(f"[Bridge] => Relaying request {request_id} (mode: {mode}) to frontend client {client_id}")
        socketio.emit('request_data_for_frontend', {
            'prompt_id': prompt_id,
            'node_id': node_id,
            'mode': mode,
            'request_id': request_id
        }, room=client_id) # Emit to the specific room/SID of the frontend client / 发送到前端客户端的特定房间/SID

        # Notify user via main status / 通过主状态通知用户
        socketio.emit('status_update', {'status': f"等待前端提供数据 Waiting for frontend: {mode}"}, room=client_id)

# Register the namespace / 注册命名空间
socketio.on_namespace(BridgeNamespace(BRIDGE_NAMESPACE))


# --- Main SocketIO Events (Frontend Communication) ---
@socketio.on('connect')
def handle_connect():
    """Handles new frontend client connections."""
    """处理新的前端客户端连接。"""
    client_id = request.sid
    log.info(f"Frontend client connected: {client_id}")
    join_room(client_id) # Join a room identified by the client_id / 加入由 client_id 标识的房间
    log.info(f"Frontend client {client_id} joined room {client_id}")
    # Send initial idle status / 发送初始空闲状态
    emit('status_update', {'status': "空闲 Idle"})

@socketio.on('disconnect')
def handle_disconnect():
    """Handles frontend client disconnections."""
    """处理前端客户端断开连接。"""
    client_id = request.sid
    log.warning(f"Frontend client disconnected: {client_id}")

    # Clean up prompt mappings / 清理提示映射
    prompt_info = client_prompt_map.pop(client_id, None)
    if prompt_info:
        prompt_id = prompt_info.get('prompt_id')
        if prompt_id and prompt_client_map.get(prompt_id) == client_id:
            del prompt_client_map[prompt_id]
            log.info(f"Cleaned up prompt mapping for disconnected client {client_id}, prompt {prompt_id}")

    # Clean up pending requests initiated FOR this client / 清理为此客户端启动的待处理请求
    reqs_to_remove = [req_id for req_id, req in list(pending_node_requests.items()) if req['client_id'] == client_id]
    for req_id in reqs_to_remove:
        if req_id in pending_node_requests:
            log.warning(f"[Main] Cleaning pending request {req_id} due to frontend disconnect.")
            node_sid = pending_node_requests[req_id]['node_sid']
            # Send error response back to the waiting node via Bridge namespace / 通过桥接命名空间将错误响应发送回等待中的节点
            socketio.emit('data_response_for_node', {
                 'request_id': req_id,
                 'error': 'Frontend client disconnected before providing data.'
                 }, room=node_sid, namespace=BRIDGE_NAMESPACE) # Ensure correct namespace / 确保正确的命名空间
            del pending_node_requests[req_id]

    leave_room(client_id) # Leave the client's room / 离开客户端的房间
    log.info(f"Frontend client {client_id} left room {client_id}")


@socketio.on('provide_data_from_frontend')
def handle_provide_data(data):
    """Receives data from frontend and relays it back to the waiting NodeBridge node."""
    """接收来自前端的数据并将其转发回等待中的 NodeBridge 节点。"""
    client_id = request.sid # SID of the sending frontend client / 发送前端客户端的 SID
    request_id = data.get('request_id')
    provided_data = data.get('data') # Can be string, number, or base64 string / 可以是字符串、数字或 base64 字符串
    error_msg = data.get('error') # Frontend might send back an error / 前端可能会发回错误
    mode = data.get('mode') # Get mode for logging / 获取模式用于日志记录

    log.info(f"[Main] <= Received data provision from frontend {client_id} for req_id={request_id}, mode={mode}")

    # Find the corresponding pending request using request_id / 使用 request_id 查找对应的待处理请求
    req_info = pending_node_requests.get(request_id)

    if req_info:
        # Verify the data came from the correct client / 验证数据是否来自正确的客户端
        if req_info['client_id'] != client_id:
            log.warning(f"[Main] Data received for request {request_id} from wrong client {client_id} (expected {req_info['client_id']}). Ignoring.")
            return

        node_sid = req_info.get('node_sid')
        if node_sid:
            log.info(f"[Main] => Found pending request {request_id}. Relaying data/error to node SID {node_sid}.")
            # Prepare response payload for the node / 为节点准备响应有效负载
            response_payload = {
                'request_id': request_id,
                'data': provided_data, # Send the data provided by frontend / 发送前端提供的数据
                'error': error_msg     # Send any error reported by frontend / 发送前端报告的任何错误
            }
            # Send the response back to the specific node via the bridge namespace / 通过桥接命名空间将响应发送回特定节点
            socketio.emit('data_response_for_node', response_payload, room=node_sid, namespace=BRIDGE_NAMESPACE)

            # Remove the pending request entry after relaying / 转发后删除待处理请求条目
            if request_id in pending_node_requests:
                 del pending_node_requests[request_id]
                 log.info(f"[Main] Removed pending request {request_id}")
            else:
                 log.warning(f"[Main] Tried to remove already removed pending request {request_id}")

             # Update frontend status / 更新前端状态
            status = "数据已发送 Data Sent" if not error_msg else f"前端错误 Frontend Error: {error_msg}"
            socketio.emit('status_update', {'status': status}, room=client_id)

        else:
            log.error(f"[Main] Found pending request {request_id} but missing node SID.")
            if request_id in pending_node_requests: del pending_node_requests[request_id]
            # Notify frontend of the internal error / 通知前端内部错误
            socketio.emit('render_error', {'message': f'Internal error: Could not find node connection for request {request_id}.'}, room=client_id)

    else:
        log.warning(f"[Main] Received data for unknown or already fulfilled request (req_id: {request_id}). Might be late response.")
        # Optionally notify the client / 可选地通知客户端
        emit('status_update', {'status': f"收到过时响应 Received late/unknown response for {request_id}"}, room=client_id)


# --- Flask Routes ---
@app.route('/')
def index():
    """Serves the main HTML page."""
    """提供主 HTML 页面。"""
    # Inject comfyui api port into the template for frontend use / 将 comfyui api 端口注入模板供前端使用
    comfyui_port_str = COMFYUI_API_ADDRESS.split(':')[-1] if ':' in COMFYUI_API_ADDRESS else '8188'
    try:
        comfyui_port = int(comfyui_port_str)
    except ValueError:
        comfyui_port = 8188 # Default if parsing fails / 如果解析失败则默认为 8188
        log.warning(f"Could not parse ComfyUI port from '{COMFYUI_API_ADDRESS}'. Using default {comfyui_port}.")

    log.info(f"Rendering index.html with comfyui_api_port = {comfyui_port}")
    return render_template('index.html', comfyui_api_port=comfyui_port)

@app.route('/icon.ico')
def favicon():
    """Serves the favicon."""
    """提供网站图标。"""
    return send_from_directory(app.static_folder, 'icon.ico', mimetype='image/vnd.microsoft.icon')

@app.route('/api/workflows', methods=['GET'])
def get_workflows():
    """Lists available .json workflow files from the configured directory."""
    """列出配置目录中可用的 .json 工作流文件。"""
    workflows_base_path = COMFYUI_WORKFLOWS_PATH
    log.info(f"API Request: Listing workflows from: {workflows_base_path}")

    # Validate the path / 验证路径
    if not os.path.isdir(workflows_base_path):
        log.error(f"Workflow directory not found or is not a directory: {workflows_base_path}")
        return jsonify({"error": f"Workflow directory misconfigured or not found."}), 500
    if not os.access(workflows_base_path, os.R_OK):
        log.error(f"No read permissions for workflow directory: {workflows_base_path}")
        return jsonify({"error": f"Cannot read workflow directory (permissions)."}), 500

    try:
        workflow_files = []
        # Walk through the directory recursively / 递归遍历目录
        for root, dirs, files in os.walk(workflows_base_path):
            for file in files:
                if file.endswith('.json'):
                    # Get the full absolute path / 获取完整的绝对路径
                    full_path = os.path.join(root, file)
                    # Get the relative path from the base workflows directory / 获取相对于基本工作流目录的相对路径
                    relative_path = os.path.relpath(full_path, workflows_base_path)
                    # Use forward slashes for web/API consistency / 使用正斜杠以保证 web/API 的一致性
                    workflow_files.append(relative_path.replace("\\", "/"))
        log.info(f"Found {len(workflow_files)} workflows.")
        # Sort workflows alphabetically for consistent frontend display / 按字母顺序对工作流进行排序，以保证前端显示一致
        workflow_files.sort()
        return jsonify({"workflows": workflow_files})
    except Exception as e:
        log.error(f"Error listing workflows from {workflows_base_path}: {e}", exc_info=True)
        return jsonify({"error": f"An unexpected error occurred while listing workflows."}), 500


# API endpoint to trigger workflow execution / 触发工作流执行的 API 端点
@app.route('/api/trigger_prompt', methods=['POST'])
def trigger_prompt():
    """Receives request from frontend, loads workflow, injects context, and starts ComfyUI listener."""
    """接收来自前端的请求，加载工作流，注入上下文，并启动 ComfyUI 监听器。"""
    data = request.get_json()
    if not data:
        log.error("Trigger request received no JSON data.")
        return jsonify({"success": False, "message": "无效请求格式 (Invalid request format)."}), 400

    client_id = data.get('clientId')
    if not client_id:
        log.error("Trigger request missing client ID in payload.")
        return jsonify({"success": False, "message": "缺少客户端 ID (Missing client ID)"}), 400
    log.info(f"API Request: Trigger prompt for client {client_id}")

    # Check for concurrent execution by the same client / 检查同一客户端的并发执行
    if client_id in client_prompt_map:
         active_prompt = client_prompt_map[client_id]['prompt_id']
         log.warning(f"Client {client_id} attempted concurrent prompt start (active: {active_prompt}).")
         return jsonify({"success": False, "message": "请等待上一个渲染完成 (Please wait for the previous render to complete)."}), 409 # Conflict / 冲突

    workflow_key = data.get('workflow_key') # Relative path from frontend / 来自前端的相对路径
    if not workflow_key:
        log.error(f"Trigger request from {client_id} missing 'workflow_key'.")
        return jsonify({"success": False, "message": "缺少工作流密钥 (Workflow key is required)"}), 400

    # Load workflow file safely / 安全地加载工作流文件
    workflow_data, error_msg = load_workflow_safely(workflow_key)
    if error_msg:
        status_code = 404 if "not found" in error_msg else (400 if "Invalid" in error_msg else 500)
        return jsonify({"success": False, "message": error_msg}), status_code

    try:
        prompt_id = str(uuid.uuid4())
        log.info(f"Assigned prompt ID {prompt_id} to client {client_id} for workflow '{workflow_key}'")

        # Store mappings before starting thread / 在启动线程之前存储映射
        client_prompt_map[client_id] = {'prompt_id': prompt_id, 'workflow_data': workflow_data}
        prompt_client_map[prompt_id] = client_id

        # Start ComfyUI listener thread / 启动 ComfyUI 监听器线程
        log.info(f"Starting ComfyUI listener thread for prompt {prompt_id}")
        thread = threading.Thread(target=queue_comfyui_prompt, args=(workflow_data, client_id, prompt_id), daemon=True)
        thread.start()

        # Send immediate feedback to client / 向客户端发送即时反馈
        socketio.emit('status_update', {'status': f"任务已提交 Queued: {prompt_id[:8]}..."}, room=client_id)

        return jsonify({
            "success": True,
            "message": "工作流已触发 (Workflow triggered successfully).",
            "prompt_id": prompt_id,
            })

    except Exception as e:
        log.error(f"Error processing trigger request for client {client_id}, workflow {workflow_key}: {e}", exc_info=True)
        # Clean up potentially inconsistent state / 清理可能不一致的状态
        if client_id in client_prompt_map: del client_prompt_map[client_id]
        if 'prompt_id' in locals() and prompt_id in prompt_client_map: del prompt_client_map[prompt_id]
        return jsonify({"success": False, "message": f"触发工作流时发生意外服务器错误 (An unexpected server error occurred during trigger)."}), 500


if __name__ == '__main__':
    log.info(f"Starting ComfyFlow Flask server (v4.0.0)...")
    log.info(f"Workflow Path: {COMFYUI_WORKFLOWS_PATH}")
    log.info(f"ComfyUI API Target: {COMFYUI_API_ADDRESS}")
    log.info(f"Bridge Namespace: {BRIDGE_NAMESPACE}")
    # Run with gevent server / 使用 gevent 服务器运行
    # Use host='0.0.0.0' to be accessible on the network / 使用 host='0.0.0.0' 以便在网络上访问
    # Use debug=False for production or stable testing / 在生产或稳定测试中使用 debug=False
    socketio.run(app, host='0.0.0.0', port=5000, debug=False) # Changed debug to False / 将调试更改为 False